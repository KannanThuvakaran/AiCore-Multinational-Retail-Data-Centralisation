{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse\n",
    "import tabula\n",
    "import requests\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "class DatabaseConnector:\n",
    "    def __init__(self, file=None):\n",
    "        self.file = file\n",
    "        self.db_creds = self.read_db_creds()\n",
    "        self.db_engine = self.init_db_engine()\n",
    "        self.db_table_list = self.list_db_tables()\n",
    "\n",
    "    def read_db_creds(self):\n",
    "        with open(self.file, 'r') as f:\n",
    "            db_creds = yaml.safe_load(f)\n",
    "            return db_creds\n",
    "    \n",
    "    def init_db_engine(self):\n",
    "        db_engine = create_engine(f\"postgresql://{self.db_creds['RDS_USER']}:{self.db_creds['RDS_PASSWORD']}@{self.db_creds['RDS_HOST']}:{self.db_creds['RDS_PORT']}/{self.db_creds['RDS_DATABASE']}\")\n",
    "        return db_engine\n",
    "\n",
    "    def list_db_tables(self):\n",
    "        insp = inspect(self.db_engine)\n",
    "        db_table_list = insp.get_table_names()\n",
    "        return db_table_list\n",
    "    \n",
    "    def upload_to_db(self, clean_dataframe, table_name):\n",
    "        db_to_sql = clean_dataframe.to_sql(table_name, self.db_engine, if_exists='replace', index=False)\n",
    "        return db_to_sql\n",
    "\n",
    "class DataExtractor:\n",
    "    def __init__(self, database=None):\n",
    "        self.database = database\n",
    "\n",
    "    def read_rds_table(self, table_name):\n",
    "        table_data = pd.read_sql_table(table_name, self.database).set_index('index')\n",
    "        return table_data\n",
    "\n",
    "    def retrieve_pdf_data(self, pdf_path):\n",
    "        pdf_df_page = tabula.read_pdf(pdf_path, pages='all')\n",
    "        pdf_df = pd.concat(pdf_df_page, ignore_index=True)\n",
    "        return pdf_df\n",
    "    \n",
    "    def list_number_of_stores(self, number_of_stores_endpoint, header):\n",
    "        response = requests.get(number_of_stores_endpoint, headers=header)\n",
    "        number_of_stores_data = response.json()\n",
    "        return number_of_stores_data['number_stores']\n",
    "    \n",
    "    def retrieve_stores_data(self, store_endpoint, number_of_stores, header):\n",
    "        store_df = []\n",
    "        for store_number in range(number_of_stores):\n",
    "            response = requests.get(f'{store_endpoint}{store_number}', headers=header).json()\n",
    "            store = pd.json_normalize(response)\n",
    "            store_df.append(store)\n",
    "        stores_df = pd.concat(store_df).set_index('index')\n",
    "        return stores_df\n",
    "    \n",
    "    def extract_from_s3_csv(self,s3_path, local_path):\n",
    "        # Split S3 path into bucket and key\n",
    "        bucket, key = s3_path.replace('s3://', '').split('/')   \n",
    "        # Create an S3 client\n",
    "        s3 = boto3.client('s3')\n",
    "        \n",
    "        # Download the file from S3\n",
    "        s3.download_file(bucket, key, local_path)\n",
    "        \n",
    "        # Read the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(local_path, index_col=0)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def extract_from_s3_json(self,s3_path, local_path):\n",
    "        # Split S3 path into bucket and key\n",
    "        bucket, key = s3_path.replace('s3://', '').split('/')   \n",
    "        # Create an S3 client\n",
    "        s3 = boto3.client('s3')\n",
    "        \n",
    "        # Download the file from S3\n",
    "        s3.download_file(bucket, key, local_path)\n",
    "        \n",
    "        # Read the CSV file into a pandas DataFrame\n",
    "        df = pd.read_json(local_path)\n",
    "        return df\n",
    "    \n",
    "class DataCleaning:\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def clean_user_data(self):\n",
    "        # Remove NULL values and duplicates\n",
    "        self.dataframe = self.dataframe.dropna().drop_duplicates()\n",
    "\n",
    "        # Clean country code\n",
    "        self.dataframe['country_code'] = self.dataframe['country_code'].replace('GGB', 'GB')\n",
    "        self.dataframe = self.dataframe[self.dataframe['country_code'].str.len() == 2]\n",
    "\n",
    "        # Clean dates\n",
    "        self.dataframe.loc[:,'date_of_birth'] = pd.to_datetime(self.dataframe['date_of_birth'].apply(parse))\n",
    "        self.dataframe.loc[:,'join_date'] = pd.to_datetime(self.dataframe['join_date'].apply(parse))\n",
    "\n",
    "        # Clean phone numbers\n",
    "        regex = '^(\\(?\\+?[0-9]*\\)?)?[0-9_\\- \\(\\)]*$'\n",
    "        self.dataframe.loc[:,'phone_number'] = self.dataframe['phone_number'].str.replace('(0)', '', regex=False)\n",
    "        self.dataframe.loc[:,'phone_number'] = self.dataframe['phone_number'].replace(r'\\D+', '', regex=True)\n",
    "\n",
    "        return self.dataframe\n",
    "\n",
    "    def clean_card_data(self):\n",
    "        card_provider_list = ['Diners Club / Carte Blanche', 'American Express', 'JCB 16 digit',\n",
    "                             'JCB 15 digit', 'Maestro', 'Mastercard', 'Discover',\n",
    "                             'VISA 19 digit', 'VISA 16 digit', 'VISA 13 digit']\n",
    "\n",
    "        # Filter card data based on card providers\n",
    "        self.dataframe = self.dataframe[self.dataframe['card_provider'].isin(card_provider_list)]\n",
    "\n",
    "        # Clean and format date columns\n",
    "        self.dataframe.loc[:,'expiry_date'] = pd.to_datetime(self.dataframe['expiry_date'], errors='coerce', format='%m/%y')\n",
    "        self.dataframe.loc[:,'date_payment_confirmed'] = pd.to_datetime(self.dataframe['date_payment_confirmed'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "        # Drop NULL values and duplicates\n",
    "        self.dataframe = self.dataframe.dropna().drop_duplicates()\n",
    "\n",
    "        return self.dataframe\n",
    "\n",
    "    def clean_store_data(self):\n",
    "        self.dataframe = self.dataframe[self.dataframe['country_code'].str.len() == 2]\n",
    "        self.dataframe.loc[:, 'opening_date'] = pd.to_datetime(self.dataframe['opening_date'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "        self.dataframe.loc[:, 'continent'] = self.dataframe['continent'].replace(['eeEurope', 'eeAmerica'], ['Europe', 'America'])\n",
    "\n",
    "        self.dataframe = self.dataframe.drop(columns='lat')\n",
    "        self.dataframe['staff_numbers'] = self.dataframe['staff_numbers'].apply(lambda x: \"\".join(filter(str.isdigit, str(x))))\n",
    "\n",
    "        self.dataframe = self.dataframe.dropna().drop_duplicates()\n",
    "\n",
    "        return self.dataframe\n",
    "        \n",
    "    def convert_product_weights(self):\n",
    "        replacements = {\n",
    "            'kg': '',\n",
    "            'g': '/1000',\n",
    "            'ml': '/1000',\n",
    "            'x': '*',\n",
    "            'oz': '/35.274',\n",
    "            '77/1000 .': '77/1000'\n",
    "        }\n",
    "    \n",
    "        self.dataframe['weight'] = self.dataframe['weight'].replace(replacements, regex=True)\n",
    "        self.dataframe['weight'] = self.dataframe['weight'].str.replace('77/1000 .', '77/1000', regex=True)\n",
    "        self.dataframe['weight'] = self.dataframe['weight'].apply(lambda x: eval(str(x))).astype(float)\n",
    "        return self.dataframe\n",
    "\n",
    "    def clean_products_data(self):\n",
    "        self.dataframe.loc[:,'removed'] = self.dataframe['removed'].str.replace('Still_avaliable', 'Still_available')\n",
    "        self.dataframe = self.dataframe[self.dataframe['removed'].isin(['Still_available', 'Removed'])]\n",
    "        self.dataframe = self.convert_product_weights()\n",
    "        return self.dataframe\n",
    "    \n",
    "    def clean_orders_data(self):\n",
    "        self.dataframe = self.dataframe.drop(columns=['level_0', 'first_name', 'last_name', '1'])\n",
    "        self.dataframe = self.dataframe.dropna().drop_duplicates()\n",
    "        return self.dataframe\n",
    "    \n",
    "    def clean_date_times(self):\n",
    "        self.dataframe = self.dataframe[self.dataframe['day'].apply(lambda x: len(str(x)) <= 2)]\n",
    "        self.dataframe = self.dataframe.dropna().drop_duplicates()\n",
    "        return self.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_database = DatabaseConnector(file='db_creds.yaml').init_db_engine()\n",
    "user_data_df = DataExtractor(yaml_database).read_rds_table('legacy_users')\n",
    "cleaned_df = DataCleaning(user_data_df).clean_user_data()\n",
    "user_data_to_sql = DatabaseConnector(file='sales_data_creds.yaml').upload_to_db(cleaned_df, 'dim_users')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = DataExtractor().retrieve_pdf_data(\"https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf\")\n",
    "data_pdf = DataCleaning(dataframe=pdf_file).clean_card_data()\n",
    "card_details_to_sql = DatabaseConnector(file='sales_data_creds.yaml').upload_to_db(data_pdf, 'dim_card_details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'x-api-key': 'yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX'}\n",
    "\n",
    "number_of_stores = DataExtractor().list_number_of_stores('https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores', headers)\n",
    "stores_data = DataExtractor().retrieve_stores_data(f'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/', number_of_stores, headers)\n",
    "\n",
    "stores_data_clean = DataCleaning(stores_data).clean_store_data()\n",
    "store_data_to_sql = DatabaseConnector('sales_data_creds.yaml').upload_to_db(stores_data_clean, 'dim_store_details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import s3fs\n",
    "#def extract_from_s3(s3_resource):\n",
    "    #df = pd.read_csv(s3_resource, index_col=0)\n",
    "    #return df\n",
    "\n",
    "#s3_df = extract_from_s3('s3://data-handling-public/products.csv')\n",
    "#s3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kthuv\\AppData\\Local\\Temp\\ipykernel_17116\\3524548519.py:139: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.dataframe['weight'] = self.dataframe['weight'].replace(replacements, regex=True)\n",
      "C:\\Users\\kthuv\\AppData\\Local\\Temp\\ipykernel_17116\\3524548519.py:140: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.dataframe['weight'] = self.dataframe['weight'].str.replace('77/1000 .', '77/1000', regex=True)\n",
      "C:\\Users\\kthuv\\AppData\\Local\\Temp\\ipykernel_17116\\3524548519.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.dataframe['weight'] = self.dataframe['weight'].apply(lambda x: eval(str(x))).astype(float)\n"
     ]
    }
   ],
   "source": [
    "# Use the local_path in the function\n",
    "s3_df = DataExtractor().extract_from_s3_csv('s3://data-handling-public/products.csv', local_path = '/Users/kthuv/AiCore/Projects/MRDC/products.csv')\n",
    "\n",
    "s3_data_df = DataCleaning(s3_df).clean_products_data()\n",
    "\n",
    "products_df = DatabaseConnector(file='sales_data_creds.yaml').upload_to_db(s3_data_df, 'dim_products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_database = DatabaseConnector(file='db_creds.yaml').init_db_engine()\n",
    "orders_data_df = DataExtractor(yaml_database).read_rds_table('orders_table')\n",
    "cleaned_order_df = DataCleaning(orders_data_df).clean_orders_data()\n",
    "orders_data_to_sql = DatabaseConnector(file='sales_data_creds.yaml').upload_to_db(cleaned_order_df, 'orders_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3_json_df = DataExtractor().extract_from_s3_json('s3://data-handling-public/date_details.json', local_path = '/Users/kthuv/AiCore/Projects/MRDC/date_details.json')\n",
    "date_times_cleaned = DataCleaning(s3_json_df).clean_date_times()\n",
    "date_times_to_sql = DatabaseConnector(file='sales_data_creds.yaml').upload_to_db(date_times_cleaned, 'dim_date_times')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

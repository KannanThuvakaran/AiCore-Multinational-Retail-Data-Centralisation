{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse\n",
    "import tabula\n",
    "import requests\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "class DatabaseConnector:\n",
    "    def __init__(self, file=None):\n",
    "        self.file = file\n",
    "        self.db_creds = self.read_db_creds()\n",
    "        self.db_engine = self.init_db_engine()\n",
    "        self.db_table_list = self.list_db_tables()\n",
    "\n",
    "    def read_db_creds(self):\n",
    "        with open(self.file, 'r') as f:\n",
    "            db_creds = yaml.safe_load(f)\n",
    "            return db_creds\n",
    "    \n",
    "    def init_db_engine(self):\n",
    "        db_engine = create_engine(f\"postgresql://{self.db_creds['RDS_USER']}:{self.db_creds['RDS_PASSWORD']}@{self.db_creds['RDS_HOST']}:{self.db_creds['RDS_PORT']}/{self.db_creds['RDS_DATABASE']}\")\n",
    "        return db_engine\n",
    "\n",
    "    def list_db_tables(self):\n",
    "        insp = inspect(self.db_engine)\n",
    "        db_table_list = insp.get_table_names()\n",
    "        return db_table_list\n",
    "    \n",
    "    def upload_to_db(self, clean_dataframe, table_name):\n",
    "        db_to_sql = clean_dataframe.to_sql(table_name, self.db_engine, if_exists='replace', index=False)\n",
    "        return db_to_sql\n",
    "\n",
    "class DataExtractor:\n",
    "    def __init__(self, database=None):\n",
    "        self.database = database\n",
    "\n",
    "    def read_rds_table(self, table_name):\n",
    "        table_data = pd.read_sql_table(table_name, self.database).set_index('index')\n",
    "        return table_data\n",
    "\n",
    "    def retrieve_pdf_data(self, pdf_path):\n",
    "        pdf_df_page = tabula.read_pdf(pdf_path, pages='all')\n",
    "        pdf_df = pd.concat(pdf_df_page, ignore_index=True)\n",
    "        return pdf_df\n",
    "    \n",
    "    def list_number_of_stores(self, number_of_stores_endpoint, header):\n",
    "        response = requests.get(number_of_stores_endpoint, headers=header)\n",
    "        number_of_stores_data = response.json()\n",
    "        return number_of_stores_data['number_stores']\n",
    "    \n",
    "    def retrieve_stores_data(self, store_endpoint, number_of_stores, header):\n",
    "        store_df = []\n",
    "        for store_number in range(number_of_stores):\n",
    "            response = requests.get(f'{store_endpoint}{store_number}', headers=header).json()\n",
    "            store = pd.json_normalize(response)\n",
    "            store_df.append(store)\n",
    "        stores_df = pd.concat(store_df).set_index('index')\n",
    "        return stores_df\n",
    "    \n",
    "    def extract_from_s3(self,s3_path, local_path):\n",
    "        # Split S3 path into bucket and key\n",
    "        bucket, key = s3_path.replace('s3://', '').split('/')   \n",
    "        # Create an S3 client\n",
    "        s3 = boto3.client('s3')\n",
    "        \n",
    "        # Download the file from S3\n",
    "        s3.download_file(bucket, key, local_path)\n",
    "        \n",
    "        # Read the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(local_path, index_col=0)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "class DataCleaning:\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def clean_user_data(self):\n",
    "        # Remove NULL values and duplicates\n",
    "        self.dataframe = self.dataframe.dropna().drop_duplicates()\n",
    "\n",
    "        # Clean country code\n",
    "        self.dataframe['country_code'] = self.dataframe['country_code'].replace('GGB', 'GB')\n",
    "        self.dataframe = self.dataframe[self.dataframe['country_code'].str.len() == 2]\n",
    "\n",
    "        # Clean dates\n",
    "        self.dataframe.loc[:,'date_of_birth'] = pd.to_datetime(self.dataframe['date_of_birth'].apply(parse))\n",
    "        self.dataframe.loc[:,'join_date'] = pd.to_datetime(self.dataframe['join_date'].apply(parse))\n",
    "\n",
    "        # Clean phone numbers\n",
    "        regex = '^(\\(?\\+?[0-9]*\\)?)?[0-9_\\- \\(\\)]*$'\n",
    "        self.dataframe.loc[:,'phone_number'] = self.dataframe['phone_number'].str.replace('(0)', '', regex=False)\n",
    "        self.dataframe.loc[:,'phone_number'] = self.dataframe['phone_number'].replace(r'\\D+', '', regex=True)\n",
    "\n",
    "        return self.dataframe\n",
    "\n",
    "    def clean_card_data(self):\n",
    "        card_provider_list = ['Diners Club / Carte Blanche', 'American Express', 'JCB 16 digit',\n",
    "                             'JCB 15 digit', 'Maestro', 'Mastercard', 'Discover',\n",
    "                             'VISA 19 digit', 'VISA 16 digit', 'VISA 13 digit']\n",
    "\n",
    "        # Filter card data based on card providers\n",
    "        self.dataframe = self.dataframe[self.dataframe['card_provider'].isin(card_provider_list)]\n",
    "\n",
    "        # Clean and format date columns\n",
    "        self.dataframe.loc[:,'expiry_date'] = pd.to_datetime(self.dataframe['expiry_date'], errors='coerce', format='%m/%y')\n",
    "        self.dataframe.loc[:,'date_payment_confirmed'] = pd.to_datetime(self.dataframe['date_payment_confirmed'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "        # Drop NULL values and duplicates\n",
    "        self.dataframe = self.dataframe.dropna().drop_duplicates()\n",
    "\n",
    "        return self.dataframe\n",
    "\n",
    "    def clean_store_data(self):\n",
    "        self.dataframe = self.dataframe[self.dataframe['country_code'].str.len() == 2]\n",
    "        self.dataframe.loc[:, 'opening_date'] = pd.to_datetime(self.dataframe['opening_date'], errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "        self.dataframe.loc[:, 'continent'] = self.dataframe['continent'].replace(['eeEurope', 'eeAmerica'], ['Europe', 'America'])\n",
    "\n",
    "        self.dataframe = self.dataframe.drop(columns='lat')\n",
    "        self.dataframe['staff_numbers'] = self.dataframe['staff_numbers'].apply(lambda x: \"\".join(filter(str.isdigit, str(x))))\n",
    "\n",
    "        self.dataframe = self.dataframe.dropna().drop_duplicates()\n",
    "\n",
    "        return self.dataframe\n",
    "        \n",
    "    def convert_product_weights(self):\n",
    "        replacements = {\n",
    "            'kg': '',\n",
    "            'g': '/1000',\n",
    "            'ml': '/1000',\n",
    "            'x': '*',\n",
    "            'oz': '/35.274',\n",
    "            '77/1000 .': '77/1000'\n",
    "        }\n",
    "    \n",
    "        self.dataframe['weight'] = self.dataframe['weight'].replace(replacements, regex=True)\n",
    "        self.dataframe['weight'] = self.dataframe['weight'].str.replace('77/1000 .', '77/1000', regex=True)\n",
    "        self.dataframe['weight'] = self.dataframe['weight'].apply(lambda x: eval(str(x))).astype(float)\n",
    "        return self.dataframe\n",
    "\n",
    "    def clean_products_data(self):\n",
    "        self.dataframe.loc[:,'removed'] = self.dataframe['removed'].str.replace('Still_avaliable', 'Still_available')\n",
    "        self.dataframe = self.dataframe[self.dataframe['removed'].isin(['Still_available', 'Removed'])]\n",
    "        self.dataframe = self.convert_product_weights()\n",
    "        return self.dataframe\n",
    "    \n",
    "    def clean_orders_data(self):\n",
    "        self.dataframe = self.dataframe.drop(columns=['level_0', 'first_name', 'last_name', '1'])\n",
    "        self.dataframe = self.dataframe.dropna().drop_duplicates()\n",
    "        return self.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_database = DatabaseConnector(file='db_creds.yaml').init_db_engine()\n",
    "user_data_df = DataExtractor(yaml_database).read_rds_table('legacy_users')\n",
    "cleaned_df = DataCleaning(user_data_df).clean_user_data()\n",
    "user_data_to_sql = DatabaseConnector(file='sales_data_creds.yaml').upload_to_db(cleaned_df, 'dim_users')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = DataExtractor().retrieve_pdf_data(\"https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf\")\n",
    "data_pdf = DataCleaning(dataframe=pdf_file).clean_card_data()\n",
    "card_details_to_sql = DatabaseConnector(file='sales_data_creds.yaml').upload_to_db(data_pdf, 'dim_card_details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'x-api-key': 'yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX'}\n",
    "\n",
    "number_of_stores = DataExtractor().list_number_of_stores('https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores', headers)\n",
    "stores_data = DataExtractor().retrieve_stores_data(f'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/', number_of_stores, headers)\n",
    "\n",
    "stores_data_clean = DataCleaning(stores_data).clean_store_data()\n",
    "store_data_to_sql = DatabaseConnector('sales_data_creds.yaml').upload_to_db(stores_data_clean, 'dim_store_details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import s3fs\n",
    "#def extract_from_s3(s3_resource):\n",
    "    #df = pd.read_csv(s3_resource, index_col=0)\n",
    "    #return df\n",
    "\n",
    "#s3_df = extract_from_s3('s3://data-handling-public/products.csv')\n",
    "#s3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kthuv\\AppData\\Local\\Temp\\ipykernel_17116\\3524548519.py:139: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.dataframe['weight'] = self.dataframe['weight'].replace(replacements, regex=True)\n",
      "C:\\Users\\kthuv\\AppData\\Local\\Temp\\ipykernel_17116\\3524548519.py:140: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.dataframe['weight'] = self.dataframe['weight'].str.replace('77/1000 .', '77/1000', regex=True)\n",
      "C:\\Users\\kthuv\\AppData\\Local\\Temp\\ipykernel_17116\\3524548519.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.dataframe['weight'] = self.dataframe['weight'].apply(lambda x: eval(str(x))).astype(float)\n"
     ]
    }
   ],
   "source": [
    "# Use the local_path in the function\n",
    "s3_df = DataExtractor().extract_from_s3('s3://data-handling-public/products.csv', local_path = '/Users/kthuv/AiCore/Projects/MRDC/products.csv')\n",
    "\n",
    "s3_data_df = DataCleaning(s3_df).clean_products_data()\n",
    "\n",
    "products_df = DatabaseConnector(file='sales_data_creds.yaml').upload_to_db(s3_data_df, 'dim_products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_database = DatabaseConnector(file='db_creds.yaml').init_db_engine()\n",
    "orders_data_df = DataExtractor(yaml_database).read_rds_table('orders_table')\n",
    "cleaned_order_df = DataCleaning(orders_data_df).clean_orders_data()\n",
    "orders_data_to_sql = DatabaseConnector(file='sales_data_creds.yaml').upload_to_db(cleaned_order_df, 'orders_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kthuv\\.conda\\envs\\mrdc\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m     \u001b[39m# destructive to chunks\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:868\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kthuv\\AiCore\\Projects\\MRDC\\practice.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kthuv/AiCore/Projects/MRDC/practice.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m s3_df \u001b[39m=\u001b[39m DataExtractor()\u001b[39m.\u001b[39;49mextract_from_s3(\u001b[39m'\u001b[39;49m\u001b[39ms3://data-handling-public/date_details.json\u001b[39;49m\u001b[39m'\u001b[39;49m, local_path \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m/Users/kthuv/AiCore/Projects/MRDC/date_details.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\kthuv\\AiCore\\Projects\\MRDC\\practice.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kthuv/AiCore/Projects/MRDC/practice.ipynb#X23sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m s3\u001b[39m.\u001b[39mdownload_file(bucket, key, local_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kthuv/AiCore/Projects/MRDC/practice.ipynb#X23sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# Read the CSV file into a pandas DataFrame\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kthuv/AiCore/Projects/MRDC/practice.ipynb#X23sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(local_path, index_col\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kthuv/AiCore/Projects/MRDC/practice.ipynb#X23sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\kthuv\\.conda\\envs\\mrdc\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\kthuv\\.conda\\envs\\mrdc\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\kthuv\\.conda\\envs\\mrdc\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m         nrows\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\kthuv\\.conda\\envs\\mrdc\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:256\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musecols \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m         columns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filter_usecols(columns)\n\u001b[1;32m--> 256\u001b[0m     col_dict \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m col_dict\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;49;00m k \u001b[39min\u001b[39;49;00m columns}\n\u001b[0;32m    258\u001b[0m     \u001b[39mreturn\u001b[39;00m index, columns, col_dict\n\u001b[0;32m    260\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kthuv\\.conda\\envs\\mrdc\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:256\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musecols \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m         columns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filter_usecols(columns)\n\u001b[1;32m--> 256\u001b[0m     col_dict \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m col_dict\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m columns}\n\u001b[0;32m    258\u001b[0m     \u001b[39mreturn\u001b[39;00m index, columns, col_dict\n\u001b[0;32m    260\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#s3_df = DataExtractor().extract_from_s3('s3://data-handling-public/date_details.json', local_path = '/Users/kthuv/AiCore/Projects/MRDC/date_details.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
